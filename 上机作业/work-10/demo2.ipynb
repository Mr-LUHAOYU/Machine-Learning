{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 疝气病症预测病马死亡率\n",
    "\n",
    "本次实验使用Logistic回归、LDA与决策树分类器来分别预测患有疝病的马的存活问题，数据集中包含了368个样本和28个特征。数据集中包含了医院检测马疝病的一些指标，有的指标比较主观，有的指标难以测量，例如马的疼痛级别。另外需要说明的是，除了部分指标主观和难以测量外，该数据还存在一个问题，数据集中有30%的值是缺失的。首先在使用三个分类器预测病马的生死之前，需要处理数据集中的数据缺失问题。\n",
    "\n",
    "## 任务1：数据预处理\n",
    "\n",
    "* 导入相关包，读取列信息；\n",
    "* 读入训练集，划分 x_data 与 y_data；\n",
    "* 删除缺失较多的列，删除标签缺失的样本；\n",
    "* 离散特征使用one-hot编码，用众数填充缺失值；\n",
    "* 连续特征使用均值填充缺失值，并对所有连续特征进行minmax 归一化；\n",
    "* 衡量所有特征之间的相关系数，进行PCA降维后得到x，再观察相关系数变化；\n",
    "* 加载测试集，划分x_data_test与y_test，再进行上述数据预处理过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T11:14:57.205925300Z",
     "start_time": "2024-04-25T11:14:57.168603900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['surgery?', 'Age', 'Hospital Number', 'rectal temperature', 'pulse', 'respiratory rate', 'temperature of extremities', 'peripheral pulse', 'mucous membranes', 'capillary refill time', \"pain - a subjective judgement of the horse's pain level\", 'peristalsis', 'abdominal distension', 'nasogastric tube', 'nasogastric reflux', 'nasogastric reflux PH', 'rectal examination - feces', 'abdomen', 'packed cell volume', 'total protein', 'abdominocentesis appearance', 'abdomcentesis total protein', 'outcome']\n"
     ]
    }
   ],
   "source": [
    "# 导入包\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 取得列信息\n",
    "with open(\"resources/horse-colic.names\", 'r', encoding='utf-8') as f:\n",
    "    data_names = f.read()\n",
    "data_columns = re.findall(r'(?<=[0-9]:).*', data_names)\n",
    "data_columns = [col.strip() for col in data_columns[:-3]]\n",
    "print(data_columns)\n",
    "\n",
    "# one-hot编码列\n",
    "col_oneHot = ['surgery?', 'Age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T11:14:57.220255900Z",
     "start_time": "2024-04-25T11:14:57.207925200Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data(data_path):\n",
    "    horse_data = pd.read_csv(data_path, header=None, sep=r\"\\s+\")\n",
    "    horse_data = horse_data.loc[:, :22]  # 只取到死亡情况\n",
    "    horse_data.columns = data_columns  # 修改列名\n",
    "    horse_data = horse_data.replace(\"?\", float('Nan')) #把问号替换为Nan\n",
    "\n",
    "    return horse_data\n",
    "\n",
    "def del_invalid(horse_data, NAN_THRES=0.5):\n",
    "    col_del = [\"Hospital Number\"]  # 医院编号与存活率关系不大，选择删除\n",
    "    # 去除缺失值占比较大的特征\n",
    "    for col in horse_data.columns:\n",
    "        if horse_data[col].isna().sum() > len(horse_data) * NAN_THRES and col not in col_del:\n",
    "            col_del.append(col)\n",
    "    \n",
    "    return col_del\n",
    "\n",
    "def preprocess(horse_data):\n",
    "    # 调用drop函数，去除horse_data的outcome列\n",
    "    # x_data = _____________\n",
    "    x_data = horse_data.drop(columns=['outcome'])\n",
    "    y_data = horse_data[['outcome']]\n",
    "\n",
    "    # outcome列修改，2（死亡）、3（被安乐死）均表示死亡，都以2表示；1表示存活\n",
    "    y_data.loc[y_data['outcome'] == '3', 'outcome'] = '2'\n",
    "    y_data['outcome'] = pd.to_numeric(y_data['outcome'], errors='coerce').astype('int32')\n",
    "\n",
    "    # 分离离散列和连续列，分别处理\n",
    "    x_discrete = x_data[list(set(col_oneHot))]\n",
    "    x_continue = x_data.drop(columns=x_discrete.columns)\n",
    "\n",
    "    # 离散特征one-hot编码\n",
    "    for col in x_discrete.columns:\n",
    "            column_onehot = pd.get_dummies(x_discrete[col], prefix=col)\n",
    "            x_discrete = x_discrete.drop(columns=col)\n",
    "            x_discrete = pd.concat([x_discrete, column_onehot], axis=1)\n",
    "\n",
    "    # 离散特征用众数填补缺失值\n",
    "    imp = SimpleImputer(strategy='most_frequent')\n",
    "    x_discrete = pd.DataFrame(imp.fit_transform(x_discrete), columns=x_discrete.columns, index=None)\n",
    "    for col in x_discrete.columns:\n",
    "        x_discrete[col] = pd.to_numeric(x_discrete[col]).astype('int32')\n",
    "\n",
    "    # 连续特征用均值填充缺失值\n",
    "    for col in x_continue.columns:\n",
    "        x_continue[col] = pd.to_numeric(x_continue[col], errors='coerce').astype(\"float64\")\n",
    "        # 使用x_continue[col]列的均值填充值为na的特征值\n",
    "        # x_continue[col] = ________________________\n",
    "        x_continue[col].fillna(x_continue[col].mean(), inplace=True)\n",
    "\n",
    "    # 对连续特征使用 minmax 归一化\n",
    "    # x_continue = ________________________\n",
    "    x_continue = (x_continue - x_continue.min()) / (x_continue.max() - x_continue.min())\n",
    "\n",
    "    x_data = pd.concat([x_discrete, x_continue], axis=1)\n",
    "\n",
    "    x_data = x_data.sort_index(axis=1)  # 对列名排序，保证验证集和训练集特征一一对应\n",
    "\n",
    "    return x_data, y_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T11:14:57.346285600Z",
     "start_time": "2024-04-25T11:14:57.216713500Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "SimpleImputer does not support data with dtype bool. Please provide either a numeric array (with a floating point or integer dtype) or categorical data represented either as an array with integer dtype or an array of string values with an object dtype.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[16], line 8\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m# 删除标签缺失的样本（存在na的样本）\u001B[39;00m\n\u001B[0;32m      7\u001B[0m horse_data \u001B[38;5;241m=\u001B[39m horse_data\u001B[38;5;241m.\u001B[39mdropna(subset\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124moutcome\u001B[39m\u001B[38;5;124m'\u001B[39m])\u001B[38;5;241m.\u001B[39mreset_index(drop\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m) \n\u001B[1;32m----> 8\u001B[0m x_data, y_data \u001B[38;5;241m=\u001B[39m \u001B[43mpreprocess\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhorse_data\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     10\u001B[0m x_data\u001B[38;5;241m.\u001B[39minfo()\n\u001B[0;32m     11\u001B[0m col_del\n",
      "Cell \u001B[1;32mIn[15], line 81\u001B[0m, in \u001B[0;36mpreprocess\u001B[1;34m(horse_data)\u001B[0m\n\u001B[0;32m     79\u001B[0m \u001B[38;5;66;03m# 离散特征用众数填补缺失值\u001B[39;00m\n\u001B[0;32m     80\u001B[0m imp \u001B[38;5;241m=\u001B[39m SimpleImputer(strategy\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmost_frequent\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m---> 81\u001B[0m x_discrete \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame(\u001B[43mimp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_discrete\u001B[49m\u001B[43m)\u001B[49m, columns\u001B[38;5;241m=\u001B[39mx_discrete\u001B[38;5;241m.\u001B[39mcolumns, index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m     82\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m col \u001B[38;5;129;01min\u001B[39;00m x_discrete\u001B[38;5;241m.\u001B[39mcolumns:\n\u001B[0;32m     83\u001B[0m     x_discrete[col] \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mto_numeric(x_discrete[col])\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mint32\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32mD:\\Python\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001B[0m, in \u001B[0;36m_wrap_method_output.<locals>.wrapped\u001B[1;34m(self, X, *args, **kwargs)\u001B[0m\n\u001B[0;32m    138\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(f)\n\u001B[0;32m    139\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapped\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m--> 140\u001B[0m     data_to_wrap \u001B[38;5;241m=\u001B[39m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    141\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data_to_wrap, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[0;32m    142\u001B[0m         \u001B[38;5;66;03m# only wrap the first output for cross decomposition\u001B[39;00m\n\u001B[0;32m    143\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m (\n\u001B[0;32m    144\u001B[0m             _wrap_data_with_container(method, data_to_wrap[\u001B[38;5;241m0\u001B[39m], X, \u001B[38;5;28mself\u001B[39m),\n\u001B[0;32m    145\u001B[0m             \u001B[38;5;241m*\u001B[39mdata_to_wrap[\u001B[38;5;241m1\u001B[39m:],\n\u001B[0;32m    146\u001B[0m         )\n",
      "File \u001B[1;32mD:\\Python\\Lib\\site-packages\\sklearn\\base.py:878\u001B[0m, in \u001B[0;36mTransformerMixin.fit_transform\u001B[1;34m(self, X, y, **fit_params)\u001B[0m\n\u001B[0;32m    874\u001B[0m \u001B[38;5;66;03m# non-optimized default implementation; override when a better\u001B[39;00m\n\u001B[0;32m    875\u001B[0m \u001B[38;5;66;03m# method is possible for a given clustering algorithm\u001B[39;00m\n\u001B[0;32m    876\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m y \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    877\u001B[0m     \u001B[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001B[39;00m\n\u001B[1;32m--> 878\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfit_params\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mtransform(X)\n\u001B[0;32m    879\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    880\u001B[0m     \u001B[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001B[39;00m\n\u001B[0;32m    881\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfit(X, y, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_params)\u001B[38;5;241m.\u001B[39mtransform(X)\n",
      "File \u001B[1;32mD:\\Python\\Lib\\site-packages\\sklearn\\impute\\_base.py:390\u001B[0m, in \u001B[0;36mSimpleImputer.fit\u001B[1;34m(self, X, y)\u001B[0m\n\u001B[0;32m    381\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdeprecated\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    382\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m    383\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mverbose\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m parameter was deprecated in version \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    384\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m1.1 and will be removed in 1.3. A warning will \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    387\u001B[0m         \u001B[38;5;167;01mFutureWarning\u001B[39;00m,\n\u001B[0;32m    388\u001B[0m     )\n\u001B[1;32m--> 390\u001B[0m X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_validate_input\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43min_fit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    392\u001B[0m \u001B[38;5;66;03m# default fill_value is 0 for numerical input and \"missing_value\"\u001B[39;00m\n\u001B[0;32m    393\u001B[0m \u001B[38;5;66;03m# otherwise\u001B[39;00m\n\u001B[0;32m    394\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfill_value \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32mD:\\Python\\Lib\\site-packages\\sklearn\\impute\\_base.py:352\u001B[0m, in \u001B[0;36mSimpleImputer._validate_input\u001B[1;34m(self, X, in_fit)\u001B[0m\n\u001B[0;32m    350\u001B[0m _check_inputs_dtype(X, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmissing_values)\n\u001B[0;32m    351\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m X\u001B[38;5;241m.\u001B[39mdtype\u001B[38;5;241m.\u001B[39mkind \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mi\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mu\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mO\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m--> 352\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    353\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSimpleImputer does not support data with dtype \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    354\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m. Please provide either a numeric array (with\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    355\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m a floating point or integer dtype) or \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    356\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcategorical data represented either as an array \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    357\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwith integer dtype or an array of string values \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    358\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwith an object dtype.\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(X\u001B[38;5;241m.\u001B[39mdtype)\n\u001B[0;32m    359\u001B[0m     )\n\u001B[0;32m    361\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m X\n",
      "\u001B[1;31mValueError\u001B[0m: SimpleImputer does not support data with dtype bool. Please provide either a numeric array (with a floating point or integer dtype) or categorical data represented either as an array with integer dtype or an array of string values with an object dtype."
     ]
    }
   ],
   "source": [
    "# 加载训练集\n",
    "horse_data = load_data(\"resources/horse-colic.data\")\n",
    "# 删除缺失较多的列\n",
    "col_del = del_invalid(horse_data)\n",
    "horse_data = horse_data.drop(columns=col_del)\n",
    "# 删除标签缺失的样本（存在na的样本）\n",
    "horse_data = horse_data.dropna(subset=['outcome']).reset_index(drop=True) \n",
    "x_data, y_data = preprocess(horse_data)\n",
    "\n",
    "x_data.info()\n",
    "col_del"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T11:14:57.349288300Z",
     "start_time": "2024-04-25T11:14:57.348288300Z"
    }
   },
   "outputs": [],
   "source": [
    "# 衡量特征之间相关系数\n",
    "df_corr = x_data.corr()\n",
    "sns.heatmap(df_corr, cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# PCA主成分分析，特征降维\n",
    "# 调用PCA函数对x_data进行降维，生成12维特征的x\n",
    "# pca = ___________\n",
    "# x = pca.___________\n",
    "pca = PCA(n_components=12)\n",
    "x = pca.fit_transform(x_data)\n",
    "print(x.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-25T11:14:57.349288300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 相关系数\n",
    "df_corr = pd.DataFrame(x, index=None).corr()\n",
    "sns.heatmap(df_corr, cmap=\"YlGnBu\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-25T11:14:57.351289300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-25T11:14:57.352289700Z"
    }
   },
   "outputs": [],
   "source": [
    "# 测试集处理\n",
    "data_test = load_data(\"resources/horse-colic.test\")\n",
    "# 删除缺失较多的列\n",
    "data_test = data_test.drop(columns=col_del)\n",
    "# 删除标签缺失的样本\n",
    "data_test = data_test.dropna(subset=['outcome']).reset_index(drop=True)\n",
    "x_data_test, y_test = preprocess(data_test)\n",
    "# 将x_data_test进行PCA特征转换，生成x_test\n",
    "# x_test = pca.___________\n",
    "x_test = pca.transform(x_data_test)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 任务2：模型训练与评估\n",
    "\n",
    "* 导入相关包；\n",
    "* 调用GridSearchCV函数，使用网格搜索法寻找最优参数penalty与C；\n",
    "* 使用最优参数的逻辑回归模型进行训练，并对验证集进行预测与评估；\n",
    "* 自定义逻辑回归函数MyLogisticRegression()，进行模型训练与预测，与上述结果比较；\n",
    "* 调用sklearn中的LDA函数LinearDiscriminantAnalysis()，进行模型训练与预测，与上述结果比较；\n",
    "* 调用sklearn中的决策树分类器函数DecisionTreeClassifier()，进行模型训练与测试，与上述结果比较。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T11:14:57.359290700Z",
     "start_time": "2024-04-25T11:14:57.353290700Z"
    }
   },
   "outputs": [],
   "source": [
    "# 导入包\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1、调用sklearn中的LogisticRegression函数实现逻辑回归分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-25T11:14:57.355289800Z"
    }
   },
   "outputs": [],
   "source": [
    "# 网格搜索\n",
    "params_grid = {\n",
    "    'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "    'C':  [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "}\n",
    "\n",
    "# grid = GridSearchCV(___________, param_grid=params_grid, cv=3, verbose=1)\n",
    "grid = GridSearchCV(LogisticRegression(), param_grid=params_grid, cv=3, verbose=1)\n",
    "# 对训练样本x与y进行模型超参数的网格搜索\n",
    "# grid.___________\n",
    "grid.fit(x_data, y_data)\n",
    "\n",
    "# 最优参数\n",
    "lgr_clf = grid.best_estimator_\n",
    "\n",
    "# 逻辑回归模型训练\n",
    "# ___________\n",
    "lgr_clf.fit(x_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-25T11:14:57.356289Z"
    }
   },
   "outputs": [],
   "source": [
    "# 逻辑回归模型预测\n",
    "print(lgr_clf.score(x_test, y_test))\n",
    "y_pred_lgr = lgr_clf.predict(x_test)\n",
    "print(classification_report(y_pred_lgr, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2、自定义逻辑回归函数MyLogisticRegression()，进行模型训练与预测，与上述结果比较。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-25T11:14:57.357289300Z"
    }
   },
   "outputs": [],
   "source": [
    "# 请在下方作答\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "class MyLogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, num_iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iterations = num_iterations\n",
    "        self.theta = None\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        m, n = X.shape\n",
    "        self.theta = np.zeros((n, 1))\n",
    "\n",
    "        for i in range(self.num_iterations):\n",
    "            z = np.dot(X, self.theta)\n",
    "            h = self.sigmoid(z)\n",
    "            gradient = np.dot(X.T, (h - y)) / m\n",
    "            self.theta -= self.learning_rate * gradient\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.theta is None:\n",
    "            raise Exception(\"Model not trained yet. Please call fit() method first.\")\n",
    "        z = np.dot(X, self.theta)\n",
    "        h = self.sigmoid(z)\n",
    "        predictions = np.where(h >= 0.5, 1, 0)\n",
    "        return predictions\n",
    "# 使用MyLogisticRegression进行模型训练与预测\n",
    "my_lr = MyLogisticRegression()\n",
    "my_lr.fit(x_data, y_data)\n",
    "predictions = my_lr.predict(x_data_test)\n",
    "# 计算准确率和生成分类报告\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "report = classification_report(y_test, predictions)\n",
    "\n",
    "print(\"准确率:\", accuracy)\n",
    "print(\"分类报告:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3、调用sklearn中的LDA函数LinearDiscriminantAnalysis()，进行模型训练与预测，与上述结果比较。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-25T11:14:57.357289300Z"
    }
   },
   "outputs": [],
   "source": [
    "# LDA线性判别分析\n",
    "# 使用未进行PCA降维的20维特征x_data，进行模型训练\n",
    "# lda_clf = ___________\n",
    "# lda_clf.___________\n",
    "lda_clf = LinearDiscriminantAnalysis()\n",
    "lda_clf.fit(x_data, y_data)\n",
    "\n",
    "# 模型测试\n",
    "print(lda_clf.score(x_data_test, y_test))\n",
    "y_pred_lda = lda_clf.predict(x_data_test)\n",
    "print(classification_report(y_pred_lda, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 4、调用sklearn中的决策树分类器函数DecisionTreeClassifier()，进行模型训练与测试，与上述结果比较。\n",
    "首先分别使用经过PCA降维与未经过PCA降维的特征进行模型训练，比较结果；然后思考并尝试能够提高分类精度的策略。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-25T11:14:57.358289500Z"
    }
   },
   "outputs": [],
   "source": [
    "# 决策树分类\n",
    "# 使用未进行PCA降维的20维特征x_data, x_data_test，进行模型训练\n",
    "# dtc_clf = ___________\n",
    "# dtc_clf.___________\n",
    "dtc_clf = DecisionTreeClassifier()\n",
    "dtc_clf.fit(x_data, y_data)\n",
    "\n",
    "# 模型测试\n",
    "print(dtc_clf.score(x_test, y_test))\n",
    "y_pred_dtc = dtc_clf.predict(x_test)\n",
    "print(classification_report(y_pred_dtc, y_test))\n",
    "\n",
    "# 使用进行PCA降维的12维特征x, x_test，进行模型训练\n",
    "# dtc_clf = ___________\n",
    "# dtc_clf.___________\n",
    "dtc_clf = DecisionTreeClassifier()\n",
    "dtc_clf.fit(x, y_data)\n",
    "\n",
    "# 模型测试\n",
    "print(dtc_clf.score(x_test, y_test))\n",
    "y_pred_dtc = dtc_clf.predict(x_test)\n",
    "print(classification_report(y_pred_dtc, y_test))\n",
    "\n",
    "# 尝试能够提高分类精度的策略（选做）\n",
    "# 请在下方作答\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# 使用随机森林分类器进行模型训练与测试\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "rf_clf.fit(x_data, y_data)\n",
    "print(\"随机森林模型测试准确率:\", rf_clf.score(x_data_test, y_test))\n",
    "y_pred_rf = rf_clf.predict(x_data_test)\n",
    "print(\"随机森林模型分类报告:\\n\", classification_report(y_pred_rf, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
